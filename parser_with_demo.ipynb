{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFfcjjHog2BL",
        "outputId": "f10348b9-b267-4c29-c020-f1d313d43439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer_six-20250416-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading pdfminer_six-20250416-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pdfminer.six, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pdfminer.six-20250416\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn nltk transformers sentence-transformers pdfminer.six faiss-cpu matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEFvQvaqh9Io",
        "outputId": "2a98a43e-7a55-4d9c-b27a-33e038674069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.28.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.28.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.28.0 gradio-client-1.10.0 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "from pdfminer.high_level import extract_text\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import faiss\n",
        "import time\n",
        "\n",
        "# Download required NLTK data with better error handling\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    print(\"NLTK data downloaded successfully\")\n",
        "except Exception as nltk_err:\n",
        "    print(f\"Warning: NLTK download issue: {nltk_err}\")\n",
        "\n",
        "# Load pre-computed job embeddings\n",
        "def load_job_embeddings(job_file=\"job_embeddings.pkl\"):\n",
        "    \"\"\"Load precomputed job embeddings from file.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(job_file):\n",
        "            error_msg = f\"Error: Job embeddings file '{job_file}' not found in {os.getcwd()}\"\n",
        "            print(error_msg)\n",
        "            return None, error_msg\n",
        "\n",
        "        with open(job_file, 'rb') as f:\n",
        "            job_data = pickle.load(f)\n",
        "\n",
        "        print(f\"Successfully loaded {len(job_data)} job embeddings from {job_file}\")\n",
        "        return job_data, None\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error loading job embeddings: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return None, error_msg\n",
        "\n",
        "# Text preprocessing functions\n",
        "def clean_text(text):\n",
        "    \"\"\"Basic text cleaning.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text by cleaning and removing stopwords.\"\"\"\n",
        "    text = clean_text(text)\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        stops = set(stopwords.words('english'))\n",
        "        words = [word for word in text.split() if word not in stops and len(word) > 2]\n",
        "        return ' '.join(words)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocess_text: {e}\")\n",
        "        return text\n",
        "\n",
        "def tokenize_into_sentences(text):\n",
        "    \"\"\"Split text into sentences.\"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # First clean the text\n",
        "        text = clean_text(text)\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        # Split into sentences\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Further clean each sentence\n",
        "        sentences = [preprocess_text(sentence) for sentence in sentences if len(sentence) > 10]\n",
        "        return sentences\n",
        "    except Exception as e:\n",
        "        print(f\"Error in tokenize_into_sentences: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_resume_sections(text):\n",
        "    \"\"\"Extract relevant sections from a resume.\"\"\"\n",
        "    try:\n",
        "        # Define section names and possible next sections\n",
        "        sections = {\n",
        "            \"education\": [\"experience\", \"skills\", \"projects\", \"certifications\", \"achievements\", \"publications\"],\n",
        "            \"experience\": [\"education\", \"skills\", \"projects\", \"certifications\", \"achievements\", \"publications\"],\n",
        "            \"skills\": [\"education\", \"experience\", \"projects\", \"certifications\", \"achievements\", \"publications\"],\n",
        "            \"projects\": [\"education\", \"experience\", \"skills\", \"certifications\", \"achievements\", \"publications\"]\n",
        "        }\n",
        "\n",
        "        result = {}\n",
        "        for section, next_sections in sections.items():\n",
        "            next_sections_pattern = \"|\".join(next_sections)\n",
        "            pattern = fr\"(?i)(?:{section})\\s*[:\\-]*\\s*(.*?)(?:(?:{next_sections_pattern})|$)\"\n",
        "            match = re.search(pattern, text, re.DOTALL)\n",
        "            if match:\n",
        "                result[section] = match.group(1).strip()\n",
        "\n",
        "        # If we couldn't find structured sections, use the whole text\n",
        "        if all(not value for value in result.values()):\n",
        "            result[\"full_text\"] = text\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extract_resume_sections: {e}\")\n",
        "        return {\"full_text\": text}\n",
        "\n",
        "def extract_skills(text):\n",
        "    \"\"\"Extract skills from resume text.\"\"\"\n",
        "    try:\n",
        "        # Common technical skills\n",
        "        tech_skills = [\n",
        "            \"python\", \"java\", \"javascript\", \"c++\", \"c#\", \"ruby\", \"php\", \"html\", \"css\",\n",
        "            \"sql\", \"nosql\", \"mongodb\", \"mysql\", \"postgresql\", \"oracle\", \"react\", \"angular\",\n",
        "            \"vue\", \"node.js\", \"express\", \"django\", \"flask\", \"spring\", \"tensorflow\",\n",
        "            \"pytorch\", \"keras\", \"scikit-learn\", \"pandas\", \"numpy\", \"excel\", \"tableau\",\n",
        "            \"power bi\", \"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"jenkins\", \"git\",\n",
        "            \"machine learning\", \"deep learning\", \"nlp\", \"computer vision\", \"data science\",\n",
        "            \"data analysis\", \"data visualization\", \"big data\", \"hadoop\", \"spark\", \"r\"\n",
        "        ]\n",
        "\n",
        "        # Find skills in the text\n",
        "        text_lower = text.lower()\n",
        "        found_skills = []\n",
        "\n",
        "        for skill in tech_skills:\n",
        "            # Match whole words only\n",
        "            if re.search(r'\\b' + re.escape(skill) + r'\\b', text_lower):\n",
        "                found_skills.append(skill)\n",
        "\n",
        "        # Look for skills section\n",
        "        skills_section_pattern = re.compile(\n",
        "            r\"(?i)(?:skills|technical skills|programming languages|technologies)\\s*[:\\-]*\\s*(.+?)(?:\\n\\s*\\n|$)\",\n",
        "            re.DOTALL\n",
        "        )\n",
        "        match = skills_section_pattern.search(text)\n",
        "        if match:\n",
        "            skills_text = match.group(1)\n",
        "            # Split by common delimiters\n",
        "            section_skills = re.split(r'[;,\\n•]', skills_text)\n",
        "            # Clean up each skill\n",
        "            section_skills = [s.strip().lower() for s in section_skills if s.strip()]\n",
        "            # Add to found skills\n",
        "            found_skills.extend([s for s in section_skills if len(s) > 2 and s not in found_skills])\n",
        "\n",
        "        return found_skills\n",
        "    except Exception as e:\n",
        "        print(f\"Error in extract_skills: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_document_embedding(sentences, model):\n",
        "    \"\"\"Get document embedding by averaging sentence embeddings.\"\"\"\n",
        "    try:\n",
        "        if not sentences:\n",
        "            return np.zeros(384)  # Default embedding dimension for MiniLM-L6\n",
        "\n",
        "        # Get embeddings for each sentence\n",
        "        embeddings = model.encode(sentences)\n",
        "\n",
        "        # Average the embeddings\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in get_document_embedding: {e}\")\n",
        "        return np.zeros(384)  # Return zero vector on error\n",
        "\n",
        "def process_resume(resume_file, model):\n",
        "    \"\"\"Process a resume file and extract information.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(resume_file):\n",
        "            return None, f\"Error: Resume file not found at {resume_file}\", [], None, 0\n",
        "\n",
        "        # Extract text from PDF\n",
        "        try:\n",
        "            text = extract_text(resume_file)\n",
        "            if not text or len(text.strip()) == 0:\n",
        "                return None, \"Error: Failed to extract text from PDF. The file may be empty, corrupted, or not a valid PDF.\", [], None, 0\n",
        "        except Exception as pdf_err:\n",
        "            return None, f\"Error extracting text from PDF: {str(pdf_err)}\", [], None, 0\n",
        "\n",
        "        # Extract sections\n",
        "        sections = extract_resume_sections(text)\n",
        "\n",
        "        # Extract skills\n",
        "        skills = extract_skills(text)\n",
        "\n",
        "        # Process text for embedding\n",
        "        all_sentences = tokenize_into_sentences(text)\n",
        "        if not all_sentences:\n",
        "            return text, \"Warning: Failed to extract meaningful sentences from resume text.\", skills, None, 0\n",
        "\n",
        "        # Get document embedding\n",
        "        embedding = get_document_embedding(all_sentences, model)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        return text, sections, skills, embedding, processing_time\n",
        "    except Exception as e:\n",
        "        error_trace = traceback.format_exc()\n",
        "        error_msg = f\"Error processing resume: {str(e)}\\n\\n{error_trace}\"\n",
        "        print(error_msg)\n",
        "        return None, error_msg, [], None, 0\n",
        "\n",
        "def build_faiss_index(job_data):\n",
        "    \"\"\"Build a FAISS index for fast similarity search.\"\"\"\n",
        "    try:\n",
        "        # Extract embeddings\n",
        "        job_embeddings = np.array([job[\"embedding\"] for job in job_data]).astype('float32')\n",
        "\n",
        "        # Create index\n",
        "        dimension = job_embeddings.shape[1]\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "        index.add(job_embeddings)\n",
        "\n",
        "        return index, None\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error building FAISS index: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return None, error_msg\n",
        "\n",
        "def match_resume_to_jobs_faiss(resume_embedding, job_data, job_embeddings_index, k=10):\n",
        "    \"\"\"Match a resume to jobs using FAISS index.\"\"\"\n",
        "    try:\n",
        "        # Convert embedding to proper format\n",
        "        query_vector = np.array([resume_embedding]).astype('float32')\n",
        "\n",
        "        # Search the index\n",
        "        distances, indices = job_embeddings_index.search(query_vector, k)\n",
        "\n",
        "        # Get the matching jobs\n",
        "        matches = []\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            if idx < len(job_data):  # Safeguard against index out of bounds\n",
        "                job = job_data[idx]\n",
        "                matches.append({\n",
        "                    \"job_id\": job[\"job_id\"],\n",
        "                    \"job_title\": job[\"job_title\"],\n",
        "                    \"similarity_score\": 1 / (1 + distances[0][i]),  # Convert distance to similarity score\n",
        "                    \"experience_level\": job.get(\"formatted_experience_level\", \"\"),\n",
        "                    \"location\": job.get(\"location\", \"\"),\n",
        "                    \"remote_allowed\": job.get(\"remote_allowed\", False),\n",
        "                    \"work_type\": job.get(\"work_type\", \"\")\n",
        "                })\n",
        "\n",
        "        return matches, None\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in FAISS matching: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return [], error_msg\n",
        "\n",
        "def match_resume_to_jobs_cosine(resume_embedding, job_data, k=10):\n",
        "    \"\"\"Match a resume to jobs using cosine similarity.\"\"\"\n",
        "    try:\n",
        "        # Reshape resume embedding for sklearn cosine_similarity\n",
        "        query_vector = resume_embedding.reshape(1, -1)\n",
        "\n",
        "        # Extract all job embeddings\n",
        "        job_embeddings = np.array([job[\"embedding\"] for job in job_data])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarities = cosine_similarity(query_vector, job_embeddings)[0]\n",
        "\n",
        "        # Get indices of top k matches\n",
        "        top_indices = np.argsort(similarities)[::-1][:k]\n",
        "\n",
        "        # Get the matching jobs\n",
        "        matches = []\n",
        "        for i, idx in enumerate(top_indices):\n",
        "            job = job_data[idx]\n",
        "            matches.append({\n",
        "                \"job_id\": job[\"job_id\"],\n",
        "                \"job_title\": job[\"job_title\"],\n",
        "                \"similarity_score\": similarities[idx],  # Cosine similarity score\n",
        "                \"experience_level\": job.get(\"formatted_experience_level\", \"\"),\n",
        "                \"location\": job.get(\"location\", \"\"),\n",
        "                \"remote_allowed\": job.get(\"remote_allowed\", False),\n",
        "                \"work_type\": job.get(\"work_type\", \"\")\n",
        "            })\n",
        "\n",
        "        return matches, None\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error in cosine similarity matching: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return [], error_msg\n",
        "\n",
        "def format_matches_for_display(matches):\n",
        "    \"\"\"Format matches for display in the UI.\"\"\"\n",
        "    try:\n",
        "        if not matches:\n",
        "            return \"No matches found.\"\n",
        "\n",
        "        table_rows = []\n",
        "        for i, match in enumerate(matches, 1):\n",
        "            row = [\n",
        "                i,\n",
        "                match[\"job_title\"],\n",
        "                f\"{match['similarity_score']:.4f}\",\n",
        "                match.get(\"location\", \"\"),\n",
        "                \"Yes\" if match.get(\"remote_allowed\", False) else \"No\"\n",
        "            ]\n",
        "            table_rows.append(row)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            table_rows,\n",
        "            columns=[\"Rank\", \"Job Title\", \"Similarity Score\", \"Location\", \"Remote?\"]\n",
        "        )\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error formatting matches: {str(e)}\"\n",
        "        print(error_msg)\n",
        "        return f\"Error: {error_msg}\"\n",
        "\n",
        "def recommend_jobs(resume_file):\n",
        "    \"\"\"Main function to process resume and recommend jobs.\"\"\"\n",
        "    try:\n",
        "        print(f\"\\n--- Processing resume: {resume_file} ---\")\n",
        "\n",
        "        # Load model\n",
        "        try:\n",
        "            model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "            print(\"Sentence transformer model loaded successfully\")\n",
        "        except Exception as model_err:\n",
        "            error_msg = f\"Error loading sentence transformer model: {str(model_err)}\"\n",
        "            print(error_msg)\n",
        "            return error_msg, None, None, None, None, None\n",
        "\n",
        "        # Load job embeddings\n",
        "        job_data, job_load_error = load_job_embeddings()\n",
        "        if job_data is None:\n",
        "            return job_load_error, None, None, None, None, None\n",
        "\n",
        "        # Process resume\n",
        "        text, sections_or_error, skills, embedding, processing_time = process_resume(resume_file, model)\n",
        "\n",
        "        # Check if there was an error in processing\n",
        "        if embedding is None:\n",
        "            return sections_or_error, None, None, None, None, text\n",
        "\n",
        "        # Build FAISS index\n",
        "        job_embeddings_index, faiss_error = build_faiss_index(job_data)\n",
        "        if job_embeddings_index is None:\n",
        "            return faiss_error, None, None, None, None, text\n",
        "\n",
        "        # Match using FAISS\n",
        "        faiss_start = time.time()\n",
        "        faiss_matches, faiss_error = match_resume_to_jobs_faiss(embedding, job_data, job_embeddings_index, k=10)\n",
        "        faiss_time = time.time() - faiss_start\n",
        "\n",
        "        if faiss_error:\n",
        "            return f\"FAISS matching error: {faiss_error}\", skills, None, None, None, text\n",
        "\n",
        "        # Match using cosine similarity\n",
        "        cosine_start = time.time()\n",
        "        cosine_matches, cosine_error = match_resume_to_jobs_cosine(embedding, job_data, k=10)\n",
        "        cosine_time = time.time() - cosine_start\n",
        "\n",
        "        if cosine_error:\n",
        "            return f\"Cosine similarity matching error: {cosine_error}\", skills, faiss_matches, None, None, text\n",
        "\n",
        "        # Format results for display\n",
        "        if isinstance(sections_or_error, dict):\n",
        "            sections = sections_or_error\n",
        "            resume_info = f\"**Extracted Sections:**\\n\"\n",
        "            for section, content in sections.items():\n",
        "                if len(content) > 300:\n",
        "                    content = content[:300] + \"...\"\n",
        "                resume_info += f\"- **{section.title()}**: {content}\\n\\n\"\n",
        "\n",
        "            resume_info += f\"**Processing time:** {processing_time:.2f} seconds\\n\"\n",
        "        else:\n",
        "            resume_info = sections_or_error  # It's an error message\n",
        "\n",
        "        # Format skills\n",
        "        skills_text = \", \".join(skills) if skills else \"No skills extracted\"\n",
        "\n",
        "        # Format match results\n",
        "        faiss_df = format_matches_for_display(faiss_matches)\n",
        "        cosine_df = format_matches_for_display(cosine_matches)\n",
        "\n",
        "        performance_metrics = f\"\"\"\n",
        "        **Performance Metrics:**\n",
        "        - FAISS matching time: {faiss_time:.4f} seconds\n",
        "        - Cosine similarity matching time: {cosine_time:.4f} seconds\n",
        "        - Resume processing time: {processing_time:.2f} seconds\n",
        "        - Total job embeddings: {len(job_data)}\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Processing complete.\")\n",
        "        return resume_info, skills_text, faiss_df, cosine_df, performance_metrics, text\n",
        "\n",
        "    except Exception as e:\n",
        "        error_trace = traceback.format_exc()\n",
        "        error_msg = f\"Error in recommend_jobs: {str(e)}\\n\\n{error_trace}\"\n",
        "        print(error_msg)\n",
        "        return error_msg, None, None, None, None, None\n",
        "\n",
        "# Create the Gradio interface\n",
        "def create_ui():\n",
        "    with gr.Blocks(title=\"Resume-Job Matcher Demo\") as demo:\n",
        "        gr.Markdown(\"# Resume-Job Matcher Demo\")\n",
        "        gr.Markdown(\"Upload a resume PDF to find matching jobs using both FAISS and cosine similarity methods.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Input components\n",
        "                resume_upload = gr.File(label=\"Upload Resume (PDF)\")\n",
        "                submit_btn = gr.Button(\"Find Matching Jobs\", variant=\"primary\")\n",
        "\n",
        "                # Output tabs for results\n",
        "                with gr.Accordion(\"Resume Text\", open=False):\n",
        "                    resume_text_output = gr.Textbox(label=\"Extracted Resume Text\", show_label=False)\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # Output components\n",
        "                with gr.Tab(\"Resume Information\"):\n",
        "                    resume_info = gr.Markdown(label=\"Resume Information\")\n",
        "                    skills_output = gr.Textbox(label=\"Extracted Skills\")\n",
        "\n",
        "                with gr.Tab(\"FAISS Matches\"):\n",
        "                    faiss_matches = gr.Dataframe(label=\"Job Matches (FAISS)\")\n",
        "\n",
        "                with gr.Tab(\"Cosine Similarity Matches\"):\n",
        "                    cosine_matches = gr.Dataframe(label=\"Job Matches (Cosine Similarity)\")\n",
        "\n",
        "                with gr.Tab(\"Performance Metrics\"):\n",
        "                    metrics_output = gr.Markdown(label=\"Performance Metrics\")\n",
        "\n",
        "        # Set up the submit action\n",
        "        submit_btn.click(\n",
        "            fn=recommend_jobs,\n",
        "            inputs=[resume_upload],\n",
        "            outputs=[resume_info, skills_output, faiss_matches, cosine_matches, metrics_output, resume_text_output],\n",
        "            api_name=\"process_resume\"\n",
        "        )\n",
        "\n",
        "        # Examples section removed as it depends on specific files\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the app\n",
        "if __name__ == \"__main__\":\n",
        "    # Print system information for debugging\n",
        "    print(f\"Current working directory: {os.getcwd()}\")\n",
        "    print(f\"Files in directory: {os.listdir('.')}\")\n",
        "\n",
        "    # Check for job embeddings file\n",
        "    job_file = \"job_embeddings.pkl\"\n",
        "    if os.path.exists(job_file):\n",
        "        print(f\"Job embeddings file found: {job_file}\")\n",
        "        file_size = os.path.getsize(job_file) / (1024 * 1024)  # Size in MB\n",
        "        print(f\"File size: {file_size:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"WARNING: Job embeddings file not found: {job_file}\")\n",
        "\n",
        "    # Launch the demo\n",
        "    demo = create_ui()\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "s7JQ-KNjl1zu",
        "outputId": "b896cd6c-5e04-458a-9201-a94616131ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK data downloaded successfully\n",
            "Current working directory: /content\n",
            "Files in directory: ['.config', 'job_embeddings.pkl', 'sample_data']\n",
            "Job embeddings file found: job_embeddings.pkl\n",
            "File size: 31.14 MB\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://24c2b300d0c7aad58c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://24c2b300d0c7aad58c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}